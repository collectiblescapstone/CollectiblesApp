{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adaf035b",
   "metadata": {},
   "source": [
    "# YOlO experimentation\n",
    "\n",
    "This notebook provides the code to create training data, and train the ultralytics YOLO model to produce oriented bounding boxes (OBB) around pokemon cards in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac18a4c",
   "metadata": {},
   "source": [
    "### notes\n",
    "\n",
    "ONNX format - potential increased execution speed, can be used by many libraries https://docs.ultralytics.com/integrations/onnx/\n",
    "\n",
    "\n",
    "possible dataset improvements\n",
    "\n",
    "- cut out random shapes from around the edge\n",
    "- glare that extends beyond the card\n",
    "- colored rectangles around card to emulate sleeves\n",
    "- tight grids to simulate binders\n",
    "- higher frequency of full art/full card designs\n",
    "- incomplete cards that clip off screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ba494",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceaefc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ultralytics in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (8.4.7)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\program files\\python310\\lib\\site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch<2.10,>=1.8.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (2.9.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (0.24.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (6.1.1)\n",
      "Requirement already satisfied: polars>=0.20.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (1.37.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: polars-runtime-32==1.37.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from polars>=0.20.0->ultralytics) (1.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: filelock in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.5)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from sympy>=1.13.3->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rsninja\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics \"onnx>=1.12.0,<2.0.0\" \"onnxslim>=0.1.71\" onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c94672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import urllib\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageStat, ImageEnhance, ImageDraw, ImageFilter\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189b61e",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc7d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dataset settings here\n",
    "\n",
    "DATASET_PATH = \"../datasets/YOLO_training\"\n",
    "OBJECTS_PATH = \"../datasets/pokemon/data/images\" # the objects you want the model to detect\n",
    "OBJECTS_HEIGHT = 825\n",
    "OBJECTS_WIDTH = 600\n",
    "OBJECTS_ASPECT_RATIO = OBJECTS_HEIGHT / OBJECTS_WIDTH\n",
    "IMAGE_DIMENSION = 640\n",
    "\n",
    "# dataset generation settings\n",
    "AMOUNT_TRAIN = 500\n",
    "AMOUNT_VAL = 25\n",
    "MAX_CARDS_PER_IMAGE = 9\n",
    "ALLOW_OVERLAP = False\n",
    "BLACK_AND_WHITE = False\n",
    "OBSTRUCTIONS = False\n",
    "\n",
    "# training settings\n",
    "EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54c90c",
   "metadata": {},
   "source": [
    "### download images to use as backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565bba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background images ready.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "output_path = DATASET_PATH + \"/backgrounds/val2017.zip\" \n",
    "\n",
    "# create directory\n",
    "if not os.path.exists(os.path.dirname(output_path)):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "if not os.path.exists(DATASET_PATH + \"/backgrounds/val2017/\"):\n",
    "    # download and extract\n",
    "    print(\"Downloading background images...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(\"Extracting\")\n",
    "    with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASET_PATH + \"/backgrounds/\")\n",
    "\n",
    "    # cleanup\n",
    "    os.remove(output_path)\n",
    "print(\"Background images ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877b2db",
   "metadata": {},
   "source": [
    "### Remove empty directories from objects path\n",
    "\n",
    "this is done since we select random directories to find random images, and this breaks if empty directories exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0507f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all empty directories in OBJECTS_PATH\n",
    "for dirpath, dirnames, filenames in os.walk(OBJECTS_PATH, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        os.rmdir(dirpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5e9a2",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc46af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_background() -> str:\n",
    "    bg_path = DATASET_PATH + \"/backgrounds/val2017/\"\n",
    "    bg_image = random.choice(os.listdir(bg_path))\n",
    "    return bg_path + bg_image\n",
    "\n",
    "def load_random_card() -> str:\n",
    "    # images may be in subdirectories\n",
    "    path = random.choice(os.listdir(OBJECTS_PATH))\n",
    "    while os.path.isdir(os.path.join(OBJECTS_PATH, path)):\n",
    "        path = os.path.join(path, random.choice(os.listdir(os.path.join(OBJECTS_PATH, path))))\n",
    "        # check if directory is empty\n",
    "        if os.path.isdir(path) and len(os.listdir(path)) == 0:\n",
    "            # return the first card that is downloaded in case all of the cards haven't been downloaded yet\n",
    "            return \"../datasets/pokemon/data/images/base1/base1-1.jpg\"\n",
    "    return os.path.join(OBJECTS_PATH, path)\n",
    "\n",
    "def normalize_homography(H):\n",
    "    \"\"\"Normalize so H[2,2] == 1\"\"\"\n",
    "    return H / H[2, 2]\n",
    "\n",
    "def compose(*matrices):\n",
    "    \"\"\"\n",
    "    Compose transforms left → right.\n",
    "    compose(A, B, C) means: C @ B @ A\n",
    "    \"\"\"\n",
    "    H = np.eye(3)\n",
    "    for M in matrices:\n",
    "        H = M @ H\n",
    "    return normalize_homography(H)\n",
    "\n",
    "def translate(tx, ty):\n",
    "    return np.array([\n",
    "        [1, 0, tx],\n",
    "        [0, 1, ty],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "def rotate(theta_rad, center=None):\n",
    "    c, s = math.cos(theta_rad), math.sin(theta_rad)\n",
    "    R = np.array([\n",
    "        [ c, -s, 0],\n",
    "        [ s,  c, 0],\n",
    "        [ 0,  0, 1]\n",
    "    ], dtype=float)\n",
    "\n",
    "    if center is None:\n",
    "        return R\n",
    "\n",
    "    cx, cy = center\n",
    "    return compose(\n",
    "        translate(-cx, -cy),\n",
    "        R,\n",
    "        translate(cx, cy)\n",
    "    )\n",
    "\n",
    "def perspective_from_corners(src_pts, dst_pts):\n",
    "    \"\"\"\n",
    "    src_pts, dst_pts: lists of 4 (x, y) pairs\n",
    "    Order must be consistent (e.g. TL, TR, BR, BL)\n",
    "    \"\"\"\n",
    "    A = []\n",
    "    b = []\n",
    "\n",
    "    for (x, y), (u, v) in zip(src_pts, dst_pts):\n",
    "        A.append([x, y, 1, 0, 0, 0, -u*x, -u*y])\n",
    "        A.append([0, 0, 0, x, y, 1, -v*x, -v*y])\n",
    "        b.append(u)\n",
    "        b.append(v)\n",
    "\n",
    "    A = np.array(A, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "\n",
    "    h = np.linalg.solve(A, b)\n",
    "\n",
    "    H = np.array([\n",
    "        [h[0], h[1], h[2]],\n",
    "        [h[3], h[4], h[5]],\n",
    "        [h[6], h[7], 1.0]\n",
    "    ])\n",
    "\n",
    "    return normalize_homography(H)\n",
    "\n",
    "def to_pillow_perspective(H):\n",
    "    \"\"\"\n",
    "    Convert 3x3 homography to Pillow's 8-tuple\n",
    "    \"\"\"\n",
    "    H = normalize_homography(H)\n",
    "    return (\n",
    "        H[0,0], H[0,1], H[0,2],\n",
    "        H[1,0], H[1,1], H[1,2],\n",
    "        H[2,0], H[2,1],\n",
    "    )\n",
    "\n",
    "def get_corners_after_transform(start_w, start_h, H: np.ndarray) -> list:\n",
    "    corners = [\n",
    "        (0, 0),\n",
    "        (start_w, 0),\n",
    "        (start_w, start_h),\n",
    "        (0, start_h)\n",
    "    ]\n",
    "    transformed_corners = []\n",
    "    for x, y in corners:\n",
    "        vec = np.array([x, y, 1], dtype=float)\n",
    "        tx, ty, tz = H @ vec\n",
    "        transformed_corners.append((float(tx / tz), float(ty / tz)))\n",
    "    return transformed_corners\n",
    "\n",
    "def mesh_distort(img: Image.Image) -> Image.Image:\n",
    "    np_img = np.array(img)\n",
    "    rgb = np_img[..., :3]\n",
    "    alpha = np_img[..., 3]\n",
    "\n",
    "    cv_rgb = cv2.cvtColor(rgb, cv2.COLOR_RGBA2RGB)\n",
    "    h, w = cv_rgb.shape[:2]\n",
    "\n",
    "    focal_length = w\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0, w / 2],\n",
    "        [0, focal_length, h / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=\"float32\")\n",
    "\n",
    "    randodist_coeffs = np.array([\n",
    "        random.uniform(-0.05, 0.05),   # k1\n",
    "        random.uniform(-0.02, 0.02),   # k2\n",
    "        random.uniform(-0.005, 0.005), # p1\n",
    "        random.uniform(-0.005, 0.005), # p2\n",
    "        random.uniform(-0.01, 0.01)    # k3\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    new_camera_mtx, _ = cv2.getOptimalNewCameraMatrix(\n",
    "        camera_matrix,\n",
    "        randodist_coeffs,\n",
    "        (w, h),\n",
    "        alpha=0\n",
    "    )\n",
    "\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(\n",
    "        camera_matrix,\n",
    "        randodist_coeffs,\n",
    "        None,\n",
    "        new_camera_mtx,\n",
    "        (w, h),\n",
    "        cv2.CV_32FC1\n",
    "    )\n",
    "\n",
    "    distorted_rgb = cv2.remap(\n",
    "        cv_rgb,\n",
    "        map1,\n",
    "        map2,\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=(0, 0, 0)\n",
    "    )\n",
    "\n",
    "    distorted_alpha = cv2.remap(\n",
    "        alpha,\n",
    "        map1,\n",
    "        map2,\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=0\n",
    "    )\n",
    "\n",
    "    # Stack channels\n",
    "    rgba = np.dstack([distorted_rgb, distorted_alpha])\n",
    "\n",
    "    # find border pixels (non-zero pixels beside zero pixels or edge of image)\n",
    "    border_mask = np.zeros((h, w), dtype=bool)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            if distorted_alpha[y, x] == 0:\n",
    "                continue\n",
    "            if (x == 0 or distorted_alpha[y, x-1] == 0) or \\\n",
    "               (x == w-1 or distorted_alpha[y, x+1] == 0) or \\\n",
    "               (y == 0 or distorted_alpha[y-1, x] == 0) or \\\n",
    "               (y == h-1 or distorted_alpha[y+1, x] == 0):\n",
    "                border_mask[y, x] = True\n",
    "    \n",
    "    # expand mask to neighboring pixels to ensure no gaps\n",
    "    kernel = np.ones((3, 3), dtype=np.uint8)\n",
    "    expanded_mask = cv2.dilate(border_mask.astype(np.uint8), kernel, iterations=1).astype(bool)\n",
    "    # set border pixels half transparent to create a smoother transition\n",
    "    distorted_alpha[expanded_mask] = np.minimum(distorted_alpha[expanded_mask], 128)\n",
    "    # update the distorted image with the new alpha values\n",
    "    rgba[..., 3] = distorted_alpha\n",
    "\n",
    "    # create final image with smoothed borders\n",
    "    distorted_pil = Image.fromarray(rgba, mode=\"RGBA\")\n",
    "\n",
    "    # apply blur to entire image\n",
    "    distorted_pil = distorted_pil.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1)))\n",
    "\n",
    "    return distorted_pil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d844",
   "metadata": {},
   "source": [
    "### Images creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7acad361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training dataset: 100%|██████████| 500/500 [01:24<00:00,  5.92it/s]\n",
      "Generating validation dataset: 100%|██████████| 25/25 [00:04<00:00,  5.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_test_item():\n",
    "    shapes = []\n",
    "\n",
    "    # smaller cards if more cards per image\n",
    "    card_amount = random.randint(1, MAX_CARDS_PER_IMAGE)\n",
    "    x = IMAGE_DIMENSION / math.ceil(math.sqrt(card_amount))\n",
    "    # assuming a card is rotated so its diagonal is aligned with the grid, what is the max height so the card still fits\n",
    "    max_card_size = math.floor((x * (OBJECTS_HEIGHT if OBJECTS_HEIGHT > OBJECTS_WIDTH else OBJECTS_WIDTH)) / (math.sqrt(OBJECTS_HEIGHT**2 + OBJECTS_WIDTH**2)))\n",
    "\n",
    "    # use python image library to paste cards onto background\n",
    "    \n",
    "    # load background \n",
    "    bg_image_path = load_random_background()\n",
    "    bg_image = Image.open(bg_image_path).convert(\"RGB\")\n",
    "    if BLACK_AND_WHITE:\n",
    "        bg_image = bg_image.convert(\"L\").convert(\"RGB\")\n",
    "    bg_image = bg_image.resize((IMAGE_DIMENSION, IMAGE_DIMENSION))\n",
    "\n",
    "    # calculate average brightness of background\n",
    "    bg_image_gray = bg_image.convert(\"L\")\n",
    "    stat = ImageStat.Stat(bg_image_gray)\n",
    "    bg_brightness = stat.mean[0]\n",
    "\n",
    "    # calculate image white balance\n",
    "    # stat = ImageStat.Stat(bg_image)\n",
    "    # r_avg, g_avg, b_avg = stat.mean\n",
    "    # gray_avg = (r_avg + g_avg + b_avg) / 3\n",
    "    # r_ratio = gray_avg / (r_avg + 1e-5)\n",
    "    # g_ratio = gray_avg / (g_avg + 1e-5)\n",
    "    # b_ratio = gray_avg / (b_avg + 1e-5)\n",
    "    # # bound between 0.5 and 2 to avoid extreme color shifts\n",
    "    # r_ratio = max(min(r_ratio, 2), 0.5)\n",
    "    # g_ratio = max(min(g_ratio, 2), 0.5)\n",
    "    # b_ratio = max(min(b_ratio, 2), 0.5)\n",
    "\n",
    "    # paste cards\n",
    "    for i in range(card_amount):\n",
    "        card_height = random.uniform(50, max_card_size)\n",
    "        # load card\n",
    "        card_image_path = load_random_card()\n",
    "        try:\n",
    "            card_image = Image.open(card_image_path).convert(\"RGBA\")\n",
    "        except:\n",
    "            continue\n",
    "        if BLACK_AND_WHITE:\n",
    "            card_image = card_image.convert(\"L\").convert(\"RGBA\")\n",
    "        card_image = card_image.resize((int(card_height / OBJECTS_ASPECT_RATIO), int(card_height)))\n",
    "\n",
    "        # adjust brightness to match background\n",
    "        card_image_gray = card_image.convert(\"L\")\n",
    "        stat = ImageStat.Stat(card_image_gray)\n",
    "        card_brightness = stat.mean[0]\n",
    "\n",
    "        brightness_ratio = bg_brightness / (card_brightness + 1e-5)\n",
    "        enhancer = ImageEnhance.Brightness(card_image)\n",
    "        card_image = enhancer.enhance(brightness_ratio)\n",
    "\n",
    "        # adjust white balance to match background\n",
    "        # r, g, b, a = card_image.split()\n",
    "        # r = r.point(lambda i: i * r_ratio)\n",
    "        # g = g.point(lambda i: i * g_ratio)\n",
    "        # b = b.point(lambda i: i * b_ratio)\n",
    "        # card_image = Image.merge('RGBA', (r, g, b, a))\n",
    "\n",
    "        # change black to transparent in edges\n",
    "        for y in range(card_image.height):\n",
    "            for x in range(card_image.width):\n",
    "                if x < 17 and y < 17 or x > card_image.width - 18 and y < 17 or x < 17 and y > card_image.height - 18 or x > card_image.width - 18 and y > card_image.height - 18:\n",
    "                    r, g, b, a = card_image.getpixel((x, y))\n",
    "                    if r == 0 and g == 0 and b == 0:\n",
    "                        card_image.putpixel((x, y), (0, 0, 0, 0))\n",
    "\n",
    "        # apply obstruction if enabled\n",
    "        if OBSTRUCTIONS and random.random() < 0.66:\n",
    "            draw = Image.new('RGBA', card_image.size, (0, 0, 0, 0))\n",
    "            obstruction_amount = random.randint(0, 10)\n",
    "\n",
    "            # generate random shapes of varying size, shape, brightness, and alpha\n",
    "            for _ in range(obstruction_amount):\n",
    "                obs_w = random.randint(int(card_image.width * 0.1), int(card_image.width * 0.6))\n",
    "                obs_h = random.randint(int(card_image.height * 0.1), int(card_image.height * 0.6))\n",
    "                obs_x = random.randint(0, card_image.width - obs_w)\n",
    "                obs_y = random.randint(0, card_image.height - obs_h)\n",
    "                shade = random.randint(0, 4) * 255 // 4\n",
    "                obstruction = Image.new('RGBA', (obs_w, obs_h), (shade, shade, shade, random.randint(150, 255)))\n",
    "                # remove random shapes from obstruction to make it less blocky\n",
    "                mask = Image.new('L', (obs_w, obs_h), 255)\n",
    "                for _ in range(random.randint(5, 15)):\n",
    "                    shape_w = random.randint(int(obs_w * 0.1), int(obs_w * 0.5))\n",
    "                    shape_h = random.randint(int(obs_h * 0.1), int(obs_h * 0.5))\n",
    "                    shape_x = random.randint(0, obs_w - shape_w)\n",
    "                    shape_y = random.randint(0, obs_h - shape_h)\n",
    "                    shape = Image.new('L', (shape_w, shape_h), 0)\n",
    "                    mask.paste(shape, (shape_x, shape_y))\n",
    "                obstruction.putalpha(mask)\n",
    "                obstruction = obstruction.rotate(random.uniform(0, 360), expand=True)\n",
    "                draw.paste(obstruction, (obs_x, obs_y), obstruction)\n",
    "            card_image = Image.alpha_composite(card_image, draw)\n",
    "\n",
    "\n",
    "        transformed_max_dim = int(math.ceil(math.sqrt(card_image.width**2 + card_image.height**2)))\n",
    "        # move to center of image\n",
    "        translate_mat = translate((transformed_max_dim - card_image.width) / 2, (transformed_max_dim - card_image.height) / 2)\n",
    "        # rotate card\n",
    "        rotation_mat = rotate(random.uniform(0, 2 * math.pi), center=(card_image.width / 2, card_image.height / 2))\n",
    "        # move corners inwards randomly to simulate perspective\n",
    "        adjust_factor = 0.2\n",
    "        corner_adjust = perspective_from_corners(\n",
    "            [\n",
    "                (0,0), \n",
    "                (card_image.width,0), \n",
    "                (card_image.width,card_image.height), \n",
    "                (0,card_image.height)\n",
    "            ],\n",
    "            [\n",
    "                (random.random()*adjust_factor * card_image.width, random.random()*adjust_factor * card_image.height),\n",
    "                (card_image.width - (random.random()*adjust_factor * card_image.width), random.random()*adjust_factor * card_image.height),\n",
    "                (card_image.width - (random.random()*adjust_factor * card_image.width), card_image.height - (random.random()*adjust_factor * card_image.height)),\n",
    "                (random.random()*adjust_factor * card_image.width, card_image.height - (random.random()*adjust_factor * card_image.height))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # compose transformations\n",
    "        H = compose(\n",
    "            corner_adjust,\n",
    "            rotation_mat,\n",
    "            translate_mat\n",
    "        )\n",
    "        H_inv = np.linalg.inv(H)\n",
    "\n",
    "        # cache starting dimensions\n",
    "        start_h = card_image.height\n",
    "        start_w = card_image.width\n",
    "\n",
    "        # apply perspective transform\n",
    "        card_image = card_image.transform(\n",
    "            (transformed_max_dim, transformed_max_dim),\n",
    "            Image.PERSPECTIVE,\n",
    "            (to_pillow_perspective(H_inv)),\n",
    "            resample=Image.BICUBIC,\n",
    "            fillcolor=(0, 0, 0, 0)\n",
    "        )\n",
    "\n",
    "        # apply slight lens distortion\n",
    "        card_image = mesh_distort(card_image)\n",
    "\n",
    "        # get transformed corners\n",
    "        corners = get_corners_after_transform(start_w, start_h, H)\n",
    "\n",
    "        paste_x = random.randint(0, IMAGE_DIMENSION - max_card_size)\n",
    "        paste_y = random.randint(0, IMAGE_DIMENSION - max_card_size)\n",
    "\n",
    "        # position card in grid to avoid overlap\n",
    "        if not ALLOW_OVERLAP:\n",
    "            paste_x = i % math.ceil(math.sqrt(card_amount)) * max_card_size + random.uniform(0,max_card_size - transformed_max_dim - 1)\n",
    "            paste_y = i // math.ceil(math.sqrt(card_amount)) * max_card_size + random.uniform(0,max_card_size - transformed_max_dim - 1)\n",
    "            \n",
    "            # keep in bounds\n",
    "            paste_x = max(min(paste_x, IMAGE_DIMENSION - max_card_size), 0)\n",
    "            paste_y = max(min(paste_y, IMAGE_DIMENSION - max_card_size), 0)\n",
    "\n",
    "        # paste card onto background\n",
    "        bg_image.paste(card_image, (int(paste_x), int(paste_y)), card_image)\n",
    "\n",
    "        # calculate segment shape\n",
    "        x1 = corners[0][0] + paste_x\n",
    "        y1 = corners[0][1] + paste_y\n",
    "        x2 = corners[1][0] + paste_x\n",
    "        y2 = corners[1][1] + paste_y\n",
    "        x3 = corners[2][0] + paste_x\n",
    "        y3 = corners[2][1] + paste_y\n",
    "        x4 = corners[3][0] + paste_x\n",
    "        y4 = corners[3][1] + paste_y\n",
    "\n",
    "\n",
    "        # draw bounding box for debugging\n",
    "        # draw = ImageDraw.Draw(bg_image)\n",
    "        # draw.line([(x1, y1), (x2, y2), (x3, y3), (x4, y4), (x1, y1)], fill=(255, 0, 0), width=2)\n",
    "\n",
    "        shapes.append([x1/IMAGE_DIMENSION, y1/IMAGE_DIMENSION, x2/IMAGE_DIMENSION, y2/IMAGE_DIMENSION, x3/IMAGE_DIMENSION, y3/IMAGE_DIMENSION, x4/IMAGE_DIMENSION, y4/IMAGE_DIMENSION])\n",
    "    \n",
    "    return bg_image, shapes\n",
    "\n",
    "def save_image_and_labels(image: Image.Image, shapes: list[float], index: int, train: bool = True):\n",
    "    # create directories if they don't exist\n",
    "    images_path = DATASET_PATH + (\"/images/train\" if train else \"/images/val\")\n",
    "    labels_path = DATASET_PATH + (\"/labels/train\" if train else \"/labels/val\")\n",
    "    os.makedirs(images_path, exist_ok=True)\n",
    "    os.makedirs(labels_path, exist_ok=True)\n",
    "\n",
    "    # validate segments are within bounds\n",
    "    for shape in shapes:\n",
    "        for num in shape:\n",
    "            if float(num) < 0 or float(num) > 1:\n",
    "                print(\"out of bounds card\")\n",
    "                # return\n",
    "\n",
    "    # save image\n",
    "    image_save_path = os.path.join(images_path, f\"image_{index:05d}.jpg\")\n",
    "    image.save(image_save_path)\n",
    "\n",
    "    # save labels\n",
    "    label_save_path = os.path.join(labels_path, f\"image_{index:05d}.txt\")\n",
    "    with open(label_save_path, 'w') as f:\n",
    "        for shape in shapes:\n",
    "            f.write(\"0 \" + \" \".join(map(str, shape)) + \"\\n\")\n",
    "\n",
    "for i in tqdm.tqdm(range(AMOUNT_TRAIN), desc=\"Generating training dataset\"):\n",
    "    img, shapes = create_test_item()\n",
    "    save_image_and_labels(img, shapes, i, train=True)\n",
    "\n",
    "for i in tqdm.tqdm(range(AMOUNT_VAL), desc=\"Generating validation dataset\"):\n",
    "    img, shapes = create_test_item()\n",
    "    save_image_and_labels(img, shapes, i, train=False)\n",
    "\n",
    "yaml_content = f\"\"\"\n",
    "path: {DATASET_PATH}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "names:\n",
    "    0: card\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(DATASET_PATH, \"card_yolo_dataset.yaml\"), 'w') as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd0ec3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3b6ec",
   "metadata": {},
   "source": [
    "### model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0a8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo26n-seg.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d544471",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c2cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.4.14 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.4.6  Python-3.12.12 torch-2.9.0+cpu CPU (11th Gen Intel Core i5-1135G7 @ 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../datasets/YOLO_training\\card_yolo_dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo26n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\notebooks\\runs\\segment\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5, 3, True]        \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    119808  ultralytics.nn.modules.block.C3k2            [384, 128, 1, True]           \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     34304  ultralytics.nn.modules.block.C3k2            [256, 64, 1, True]            \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     95232  ultralytics.nn.modules.block.C3k2            [192, 128, 1, True]           \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    463104  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True, 0.5, True]\n",
      " 23        [16, 19, 22]  1    790431  ultralytics.nn.modules.head.Segment26        [1, 32, 64, 1, True, [64, 128, 256]]\n",
      "YOLO26n-seg summary: 309 layers, 3,053,055 parameters, 3,053,055 gradients, 10.2 GFLOPs\n",
      "\n",
      "Transferred 740/844 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 108.720.2 MB/s, size: 61.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\datasets\\YOLO_training\\labels\\train... 100 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 100/100 604.1it/s 0.2ss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\datasets\\YOLO_training\\labels\\train.cache\n",
      "WARNING cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 100/100 964.8it/s 0.1s.2s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 113.917.6 MB/s, size: 45.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\datasets\\YOLO_training\\labels\\val... 10 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 10/10 864.0it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\datasets\\YOLO_training\\labels\\val.cache\n",
      "WARNING cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100% ━━━━━━━━━━━━ 10/10 974.3it/s 0.0s\n",
      "Plotting labels to C:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\notebooks\\runs\\segment\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.002, momentum=0.9) with parameter groups 0 weight(decay=0.0), 0 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Code\\react\\CollectiblesApp\\src\\ai_dev\\notebooks\\runs\\segment\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       1/50         0G     0.7395      2.167      3.789   0.004504      5.315         27        640: 100% ━━━━━━━━━━━━ 7/7 19.7s/it 2:18.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.6s/it 3.6s\n",
      "                   all         10         59       0.01      0.508     0.0126    0.00895     0.0123      0.627     0.0341     0.0158\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       2/50         0G     0.6984      2.032      3.732   0.004503      4.191         23        640: 100% ━━━━━━━━━━━━ 7/7 25.6s/it 2:59.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 4.5s/it 4.5s\n",
      "                   all         10         59      0.011      0.559     0.0177     0.0119     0.0133      0.678     0.0293     0.0182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       3/50         0G     0.6487      1.736      3.638   0.004109      3.605         31        640: 100% ━━━━━━━━━━━━ 7/7 29.7s/it 3:28.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.3s/it 2.3s\n",
      "                   all         10         59     0.0113      0.576     0.0257     0.0198     0.0143      0.729     0.0409     0.0273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       4/50         0G     0.6131      1.614      3.608   0.003974        2.3         28        640: 100% ━━━━━━━━━━━━ 7/7 23.7s/it 2:46.6ss9\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all         10         59     0.0117      0.593     0.0545     0.0351     0.0147      0.746     0.0542     0.0453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       5/50         0G     0.6311      1.479      3.628   0.003664     0.2269         30        640: 100% ━━━━━━━━━━━━ 7/7 24.7s/it 2:53.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.1s/it 2.1s\n",
      "                   all         10         59      0.013      0.661      0.106     0.0797     0.0157      0.797      0.097     0.0854\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       6/50         0G      0.564      1.329      3.483   0.003334    0.00305         27        640: 100% ━━━━━━━━━━━━ 7/7 29.3s/it 3:25.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all         10         59     0.0133      0.678      0.155      0.138      0.016      0.814      0.162       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       7/50         0G     0.5261      1.073      3.397   0.003165   0.000161         19        640: 100% ━━━━━━━━━━━━ 7/7 12060.3s/it 23:27:02\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.6s/it 1.6s\n",
      "                   all         10         59      0.015      0.763       0.25      0.232     0.0167      0.847      0.258      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       8/50         0G     0.5246     0.9045      3.267   0.003081  2.411e-05         17        640: 100% ━━━━━━━━━━━━ 7/7 23.5s/it 2:45.5ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.1s/it 2.1s\n",
      "                   all         10         59     0.0153       0.78      0.296      0.277     0.0167      0.847        0.3      0.288\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K       9/50         0G      0.516     0.8205      3.081   0.003069  9.456e-06         27        640: 100% ━━━━━━━━━━━━ 7/7 25.3s/it 2:57.4s:45\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.6s/it 2.6s\n",
      "                   all         10         59     0.0157      0.797      0.313      0.301     0.0167      0.847      0.317      0.306\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      10/50         0G      0.513     0.7837      2.827    0.00306   2.99e-06         27        640: 100% ━━━━━━━━━━━━ 7/7 27.7s/it 3:14.6ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all         10         59     0.0187      0.949       0.34      0.326      0.019      0.966       0.34      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      11/50         0G     0.5221     0.7063      2.545   0.002967  1.573e-06         32        640: 100% ━━━━━━━━━━━━ 7/7 45.7s/it 5:20.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.0s/it 3.0s\n",
      "                   all         10         59      0.542      0.576      0.617      0.574      0.542      0.576      0.615      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      12/50         0G     0.4968     0.6845      2.273   0.002951  7.544e-07         37        640: 100% ━━━━━━━━━━━━ 7/7 40.2s/it 4:42.3s:36\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.7s/it 2.7s\n",
      "                   all         10         59      0.614      0.576      0.644       0.59      0.614      0.576       0.64      0.591\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      13/50         0G     0.5111     0.6119       2.22   0.002758  4.151e-07         41        640: 100% ━━━━━━━━━━━━ 7/7 39.7s/it 4:38.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.8s/it 2.8s\n",
      "                   all         10         59       0.62      0.644      0.667      0.605       0.62      0.644      0.664      0.613\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      14/50         0G     0.4817     0.6483      2.284   0.002831  2.435e-07         29        640: 100% ━━━━━━━━━━━━ 7/7 38.1s/it 4:27.5s:24\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.4s/it 3.4s\n",
      "                   all         10         59      0.657      0.678      0.737      0.678      0.657      0.678      0.737      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      15/50         0G     0.4618     0.6138      2.135   0.002906  1.517e-07         27        640: 100% ━━━━━━━━━━━━ 7/7 39.2s/it 4:35.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all         10         59      0.621      0.712      0.693      0.645      0.621      0.712      0.693       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      16/50         0G     0.4816     0.6197      2.022    0.00267  1.038e-07         19        640: 100% ━━━━━━━━━━━━ 7/7 37.8s/it 4:24.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.8s/it 2.8s\n",
      "                   all         10         59      0.648      0.678      0.692      0.643      0.648      0.678      0.692       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      17/50         0G      0.474     0.5861        1.9   0.002842  3.193e-08         20        640: 100% ━━━━━━━━━━━━ 7/7 32.3s/it 3:46.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.4s/it 3.4s\n",
      "                   all         10         59      0.681      0.678      0.704      0.658      0.681      0.678      0.704       0.66\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      18/50         0G      0.469     0.6362       1.77   0.002671  7.983e-09         30        640: 100% ━━━━━━━━━━━━ 7/7 34.1s/it 3:59.1sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.6s/it 2.6s\n",
      "                   all         10         59      0.705       0.78      0.812      0.757      0.705       0.78      0.812      0.757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      19/50         0G      0.484       0.61      1.718   0.002791  3.991e-09         28        640: 100% ━━━━━━━━━━━━ 7/7 35.7s/it 4:10.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.5s/it 2.5s\n",
      "                   all         10         59      0.726      0.763      0.853      0.793      0.726      0.763      0.853      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      20/50         0G     0.4597     0.5965      1.547   0.002732          0         24        640: 100% ━━━━━━━━━━━━ 7/7 32.7s/it 3:49.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.4s/it 2.4s\n",
      "                   all         10         59      0.724      0.814      0.875      0.819      0.724      0.814      0.875      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      21/50         0G      0.456     0.5789      1.561   0.002602          0         34        640: 100% ━━━━━━━━━━━━ 7/7 31.6s/it 3:41.4s:04\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 5.8s/it 5.8s\n",
      "                   all         10         59      0.894      0.713      0.894      0.843      0.894      0.713      0.894      0.831\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      22/50         0G     0.4537     0.5856      1.506   0.002657          0        111        640: 71% ━━━━━━━━╸─── 5/7 1.6s/it 1:01:04<3.2ss"
     ]
    }
   ],
   "source": [
    "results = model.train(data=os.path.join(DATASET_PATH, \"card_yolo_dataset.yaml\"), epochs=EPOCHS , imgsz=IMAGE_DIMENSION, save_period=10, cache=True)\n",
    "\n",
    "# save model\n",
    "model.save(\"card_yolo.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb382a08",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a2082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.7  Python-3.12.12 torch-2.9.0+cu130 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
      "YOLO26n-seg summary (fused): 139 layers, 2,689,079 parameters, 0 gradients, 9.0 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 639.2169.1 MB/s, size: 68.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Code\\React\\CollectiblesApp\\src\\ai_dev\\datasets\\YOLO_training\\labels\\val.cache... 25 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 25/25  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.4s9.2s\n",
      "                   all         25        129      0.976      0.992      0.994      0.972      0.976      0.992      0.994      0.963\n",
      "Speed: 4.2ms preprocess, 20.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Code\\React\\CollectiblesApp\\src\\ai_dev\\notebooks\\runs\\segment\\val\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.97155])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"./card_yolo.pt\")\n",
    "\n",
    "metrics = model.val(data=os.path.join(DATASET_PATH, \"card_yolo_dataset.yaml\"))\n",
    "\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list containing mAP50-95 for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eebdc1",
   "metadata": {},
   "source": [
    "### model exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.7  Python-3.12.12 torch-2.9.0+cu130 CPU (Intel Core i5-9600K 3.70GHz)\n",
      "YOLO26n-seg summary (fused): 139 layers, 2,689,079 parameters, 0 gradients, 9.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'card_yolo.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 300, 38), (1, 32, 160, 160)) (6.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.1 opset 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\utils.py:1447: OnnxExporterWarning: Exporting to ONNX opset version 22 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 20. To use a newer opset version, consider 'torch.onnx.export(..., dynamo=True)'. \n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 22 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.82...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.4s, saved as 'card_yolo.onnx' (10.6 MB)\n",
      "\n",
      "Export complete (2.9s)\n",
      "Results saved to \u001b[1mC:\\Code\\React\\CollectiblesApp\\src\\ai_dev\\notebooks\u001b[0m\n",
      "Predict:         yolo predict task=segment model=card_yolo.onnx imgsz=640 \n",
      "Validate:        yolo val task=segment model=card_yolo.onnx imgsz=640 data=../datasets/YOLO_training\\card_yolo_dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'card_yolo.onnx'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"./card_yolo.pt\")\n",
    "\n",
    "model.export(format=\"onnx\", opset=12, dynamic=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchstuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
